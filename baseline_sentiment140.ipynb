{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff07b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696426c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run data_processing.ipynb to generate the train, val, test sets here\n",
    "# load and split train, val, test, (X, y)\n",
    "TRAIN_DATA = 'data/training.1440000.csv'\n",
    "VAL_DATA = 'data/validation.80000.csv'\n",
    "TEST_DATA = 'data/test.80000.csv'\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA)\n",
    "df_val = pd.read_csv(VAL_DATA)\n",
    "df_test = pd.read_csv(TEST_DATA)\n",
    "\n",
    "X_train, y_train = df_train['text'], df_train['target']\n",
    "X_val, y_val = df_val['text'], df_val['target']\n",
    "X_test, y_test = df_test['text'], df_test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759b985",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5d697",
   "metadata": {},
   "source": [
    "Use some naive model or cite previous paper? \n",
    "1. use some naive model - like sentiment lexicon and classify positive if most words are positive\n",
    "- pros: can replicate\n",
    "2. use previous research results\n",
    "- concerns: may not be able to find original source code to replicate baseline results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a1501",
   "metadata": {},
   "source": [
    "#### TwitrRatr - same baseline as https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97a4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : CANNOT FIND LIST OF POSITIVE/NEGATIVE WORDS MENTIONED IN THE PAPER???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d7e02",
   "metadata": {},
   "source": [
    "#### Opinion lexicon - https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/blob/master/data/opinion-lexicon-English/negative-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b78038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+',\n",
       " 'abound',\n",
       " 'abounds',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'accessable',\n",
       " 'accessible',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclamation']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSITIVE_FILE = 'data/positive-words.txt'\n",
    "positive_words = pd.read_table(POSITIVE_FILE, skiprows=34, names=['words'], encoding='ISO-8859-1')['words'].tolist()\n",
    "positive_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e9963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEGATIVE_FILE = 'data/negative-words.txt'\n",
    "negative_words = pd.read_table(NEGATIVE_FILE, skiprows=34, names=['words'], encoding='ISO-8859-1')['words'].tolist()\n",
    "negative_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4c9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split based on all punctuations possible\n",
    "def split_tweet(tweet):\n",
    "    return \"\".join((char if char.isalpha() else \" \") for char in tweet).split()\n",
    "\n",
    "def count_pos(tweet):\n",
    "    tweet_words = split_tweet(tweet)\n",
    "    return sum([1 for w in tweet_words if w in positive_words])\n",
    "\n",
    "def count_neg(tweet):\n",
    "    tweet_words = split_tweet(tweet)\n",
    "    return sum([1 for w in tweet_words if w in negative_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06d6878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USERNAME yeah sure whatevs...haha you have to admit the sweet and innocent are by far the best and a epic win '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "effaecd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pos(df_test.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233ea19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_neg(df_test.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44b3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = df_test.text.apply(lambda x: 1 if count_pos(x) - count_neg(x) >= 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4971b490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "79995    1\n",
       "79996    1\n",
       "79997    1\n",
       "79998    1\n",
       "79999    0\n",
       "Name: text, Length: 80000, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cdf11",
   "metadata": {},
   "source": [
    "Naive Bayes Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3624653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model, mode = 'val'):\n",
    "    # mode in {val, test}\n",
    "    if mode == 'test':\n",
    "        eval_x = X_test\n",
    "        eval_y = y_test\n",
    "    else: \n",
    "        eval_x = X_val\n",
    "        eval_y = y_val\n",
    "    \n",
    "    # Predict values for given dataset\n",
    "    y_pred = model.predict(eval_x)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(eval_y, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(eval_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc5f6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the raw input into a matrix of TF-IDF features\n",
    "\n",
    "# 1. train a tf-idf \n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "\n",
    "# 2. vectorize the raw inputs\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_val = vectoriser.transform(X_val)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7935fab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80     40215\n",
      "           1       0.79      0.81      0.80     39785\n",
      "\n",
      "    accuracy                           0.80     80000\n",
      "   macro avg       0.80      0.80      0.80     80000\n",
      "weighted avg       0.80      0.80      0.80     80000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NBmodel = BernoulliNB(alpha = 2)\n",
    "NBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(NBmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f82cc5",
   "metadata": {},
   "source": [
    "SVM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SVMmodel = svm.SVC(kernel='linear') # Linear Kernel\n",
    "SVMmodel.fit(X_train, y_train)\n",
    "model_Evaluate(SVMmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b518c35",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ad240",
   "metadata": {},
   "source": [
    "TODO: What to pick for evaluation metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23c00a",
   "metadata": {},
   "source": [
    "#### F1-score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad729222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14698, 25448],\n",
       "       [ 3985, 35869]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(df_test.target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c05040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6043811204758519"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(df_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed9af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e8277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
